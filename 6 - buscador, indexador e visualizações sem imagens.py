# -*- coding: utf-8 -*-
"""buscador.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yul3ohEIOYYzisBT2FxVWBUYEmzOxwpQ

#    Pré-requisitos a serem instalados:
   
  **PyPDF2** ou pdfplumber: Para leitura e extração de texto dos arquivos PDF.
  **os**: Para navegação nas pastas e localização dos arquivos PDF.
  **openpyxl, csv, ou json**: Para salvar os resultados nos formatos XLSX, CSV ou JSON.

Aqui está um exemplo básico de como isso pode ser implementado:
Passos para implementação:

#  Funções do código.
  Procurar pelas palavras-chave dentro de cada PDF.
  Registrar o nome do arquivo, página onde a palavra foi encontrada e uma amostra do texto ao redor da palavra.
  Exportar os resultados para um arquivo XLSX, CSV ou JSON.

Comando para abrir o drive com os arquivos em pdf a serem analisados
"""

from google.colab import drive
drive.mount('/content/drive')

"""Instalação de pré-requisitos

# Como o código funciona:

  1  Função extract_text_from_pdf: Extrai o texto de cada página de um arquivo PDF.
  2 Função search_keywords_in_text: Procura as palavras-chave no texto do PDF, retornando a página, a palavra encontrada e uma amostra do texto em torno dela.
  3 Funções de salvamento: Salvam os resultados em CSV, XLSX ou JSON.
  4 Função search_in_pdfs: Busca nos PDFs da pasta por palavras-chave e salva os resultados.

Passos para usar:

  **Instale as dependências**: pip install pdfplumber openpyxl.
    Substitua **folder_path** pelo caminho da pasta onde estão os PDFs.
    Substitua **keywords** pelas palavras-chave que você deseja buscar.
    Escolha o formato de saída (**CSV, XLSX ou JSON**) na variável output_format.
    Execute o script.
"""

pip install pdfplumber

pip install PyPDF2

"""# Código de busca"""

import os
import PyPDF2
import pdfplumber
import csv
import json
import openpyxl

# Função para extrair o texto de um arquivo PDF
def extract_text_from_pdf(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        text = {}
        for i, page in enumerate(pdf.pages):
            text[i+1] = page.extract_text()  # i+1 para começar a contagem da página em 1
        return text

# Função para buscar palavras-chave no texto
def search_keywords_in_text(text, keywords):
    results = []
    for page_num, page_text in text.items():
        for keyword in keywords:
            if keyword.lower() in page_text.lower():
                sample_start = max(0, page_text.lower().find(keyword.lower()) - 30)
                sample_end = min(len(page_text), page_text.lower().find(keyword.lower()) + 30)
                sample = page_text[sample_start:sample_end]
                results.append((page_num, keyword, sample))
    return results

# Função para salvar os resultados em um arquivo CSV
def save_to_csv(data, output_file):
    with open(output_file, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(["File Name", "Page Number", "Keyword", "Sample"])
        for row in data:
            writer.writerow(row)

# Função para salvar os resultados em um arquivo XLSX
def save_to_xlsx(data, output_file):
    workbook = openpyxl.Workbook()
    sheet = workbook.active
    sheet.append(["File Name", "Page Number", "Keyword", "Sample"])
    for row in data:
        sheet.append(row)
    workbook.save(output_file)

# Função para salvar os resultados em um arquivo JSON
def save_to_json(data, output_file):
    with open(output_file, mode='w', encoding='utf-8') as file:
        json.dump(data, file, indent=4)

# Função principal para buscar palavras-chave em todos os PDFs da pasta
def search_in_pdfs(folder_path, keywords, output_format):
    result_data = []
    for file_name in os.listdir(folder_path):
        if file_name.endswith('.pdf'):
            file_path = os.path.join(folder_path, file_name)
            pdf_text = extract_text_from_pdf(file_path)
            search_results = search_keywords_in_text(pdf_text, keywords)
            for page_num, keyword, sample in search_results:
                result_data.append([file_name, page_num, keyword, sample])

    # Salvar os resultados de acordo com o formato especificado
    if output_format == 'csv':
        save_to_csv(result_data, 'resultados.csv')
    elif output_format == 'xlsx':
        save_to_xlsx(result_data, 'resultados.xlsx')
    elif output_format == 'json':
        save_to_json(result_data, 'resultados.json')
    else:
        print("Formato não suportado. Escolha entre 'csv', 'xlsx' ou 'json'.")

# Configurar a pasta e as palavras-chave
folder_path = 'base/'
keywords = ['América', 'América Latina', 'Latin-América', 'hispâno américa', 'pan-américa']  # <-------------------------------- Adicione suas palavras-chave aqui
output_format = 'csv'  # Escolha entre 'csv', 'xlsx' ou 'json'

# Executar o buscador
search_in_pdfs(folder_path, keywords, output_format)

"""### Após a execussão, imprima os resultados. Na função "print", que imprimir os resultados, foi definido o métod first_row, porque não queremos ver a tabela inteira, apenas o formato"""

import csv

# Função para ler um arquivo CSV e imprimir apenas o cabeçalho e a primeira linha
def print_csv_header_and_first_row(file_path):
    with open(file_path, mode='r', encoding='utf-8') as file:
        reader = csv.reader(file)
        header = next(reader)  # Lê o cabeçalho
        first_row = next(reader)  # Lê a primeira linha de dados
        print("Cabeçalho:", header)
        print("Primeira linha:", first_row)

# Exemplo de uso - Caminho para o arquivo CSV
csv_file_path = 'resultados.csv'  # Substitua pelo caminho do seu arquivo CSV

# Imprimir o cabeçalho e a primeira linha do CSV
print_csv_header_and_first_row(csv_file_path)

"""# Observe que o nome do 'PDF' foi configurado com informações que remetem à origem do acervo (a revista do IHGB), ao número e volume. Dessa forma, é possível 'popular' a base de dados. Em vez de conter apenas o número da página, a palavra-chave e a citação, agora a base de dados inclui mais três entidades: ano, volume/tomo e nome do arquivo. Antes de avançar para o próximo passo, realize as buscas que considerar necessárias. Lembre-se de que, a cada execução, o arquivo de resultados será reescrito. Abaixo está um código aprimorado que preenche a base de dados, complementando a versão anterior."""

# @title Código que complementa as buscas
import os
import PyPDF2
import pdfplumber
import csv
import json
import openpyxl

# Função para extrair o texto de um arquivo PDF
def extract_text_from_pdf(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        text = {}
        for i, page in enumerate(pdf.pages):
            text[i+1] = page.extract_text()  # i+1 para começar a contagem da página em 1
        return text

# Função para buscar palavras-chave no texto
def search_keywords_in_text(text, keywords):
    results = []
    for page_num, page_text in text.items():
        for keyword in keywords:
            if keyword.lower() in page_text.lower():
                sample_start = max(0, page_text.lower().find(keyword.lower()) - 30)
                sample_end = min(len(page_text), page_text.lower().find(keyword.lower()) + 30)
                sample = page_text[sample_start:sample_end]
                results.append((page_num, keyword, sample))
    return results

# Função para salvar os resultados em um arquivo CSV sem sobrescrever
def save_to_csv(data, output_file):
    file_exists = os.path.isfile(output_file)
    with open(output_file, mode='a', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        if not file_exists:
            writer.writerow(["File Name", "Page Number", "Keyword", "Sample"])  # Cabeçalho apenas se o arquivo não existir
        for row in data:
            writer.writerow(row)

# Função para salvar os resultados em um arquivo XLSX sem sobrescrever
def save_to_xlsx(data, output_file):
    if os.path.exists(output_file):
        workbook = openpyxl.load_workbook(output_file)
        sheet = workbook.active
    else:
        workbook = openpyxl.Workbook()
        sheet = workbook.active
        sheet.append(["File Name", "Page Number", "Keyword", "Sample"])  # Adicionar o cabeçalho se for um novo arquivo

    for row in data:
        sheet.append(row)

    workbook.save(output_file)

# Função para salvar os resultados em um arquivo JSON sem sobrescrever
def save_to_json(data, output_file):
    if os.path.exists(output_file):
        with open(output_file, mode='r', encoding='utf-8') as file:
            existing_data = json.load(file)
    else:
        existing_data = []

    existing_data.extend(data)

    with open(output_file, mode='w', encoding='utf-8') as file:
        json.dump(existing_data, file, indent=4)

# Função principal para buscar palavras-chave em todos os PDFs da pasta
def search_in_pdfs(folder_path, keywords, output_format):
    result_data = []
    for file_name in os.listdir(folder_path):
        if file_name.endswith('.pdf'):
            file_path = os.path.join(folder_path, file_name)
            pdf_text = extract_text_from_pdf(file_path)
            search_results = search_keywords_in_text(pdf_text, keywords)
            for page_num, keyword, sample in search_results:
                result_data.append([file_name, page_num, keyword, sample])

    # Salvar os resultados de acordo com o formato especificado
    if output_format == 'csv':
        save_to_csv(result_data, 'resultados.csv')
    elif output_format == 'xlsx':
        save_to_xlsx(result_data, 'resultados.xlsx')
    elif output_format == 'json':
        save_to_json(result_data, 'resultados.json')
    else:
        print("Formato não suportado. Escolha entre 'csv', 'xlsx' ou 'json'.")

# Configurar a pasta e as palavras-chave
folder_path = 'base/'  # Substitua pelo caminho da pasta com os PDFs
keywords = ['América', 'América Latina', 'Latin-América', 'hispâno américa', 'pan-américa']  # <-------------------------------- Adicione suas palavras-chave aqui
output_format = 'csv'  # Escolha entre 'csv', 'xlsx' ou 'json'

# Executar o buscador
search_in_pdfs(folder_path, keywords, output_format)

"""# O código a seguir modelará os dados para expandir as informações disponíveis para a hermeneutica historiadora. Se a pesquisa trabalha com livros, cabe definir o nome dos arquivos como SOBRENOME, Autor. Nome da Obra. Cidade: editora, ano. Assim, aumenta-se a possibilidade de cruzamento de informações."""

import csv
import os
import re
import pandas as pd

# Função para extrair origem, ano e tomo a partir do nome do arquivo no formato rihgb1925t0097.pdf
def extract_entities_from_filename(file_name):
    # Remover a extensão do arquivo
    base_name = os.path.splitext(file_name)[0]  # Remove '.pdf'

    # Usar expressão regular para extrair as partes do nome do arquivo
    match = re.match(r'([a-zA-Z]+)(\d{4})t(\d+)', base_name)

    if match:
        origem = match.group(1)  # 'rihgb'
        ano = match.group(2)  # '1925'
        tomo = match.group(3)  # '0097'
    else:
        origem, ano, tomo = "Desconhecido", "Desconhecido", "Desconhecido"

    return origem, ano, tomo

# Função para ler o arquivo CSV, adicionar as colunas e imprimir a primeira linha formatada como tabela
def update_csv_with_new_entities_and_display_table(file_path):
    updated_rows = []

    # Ler o arquivo CSV original
    with open(file_path, mode='r', encoding='utf-8') as file:
        reader = csv.reader(file)
        header = next(reader)

        # Adicionar novas colunas ao cabeçalho, se ainda não existirem
        if "Origem" not in header and "Ano" not in header and "Tomo" not in header:
            header.extend(["Origem", "Ano", "Tomo"])
        updated_rows.append(header)

        # Processar cada linha e adicionar as novas entidades
        for row in reader:
            if len(row) > 0:  # Garantir que a linha não esteja vazia
                file_name = row[0]  # Supondo que 'File Name' está na primeira coluna
                origem, ano, tomo = extract_entities_from_filename(file_name)
                row.extend([origem, ano, tomo])
                updated_rows.append(row)

    # Reescrever o arquivo CSV com as novas colunas
    with open(file_path, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerows(updated_rows)

    # Exibir a primeira linha formatada como tabela
    if len(updated_rows) > 1:
        df = pd.DataFrame([updated_rows[1]], columns=updated_rows[0])
        print(df)

# Exemplo de uso - Caminho para o arquivo CSV
csv_file_path = 'resultados.csv'  # Substitua pelo caminho do seu arquivo CSV

# Executar a função para atualizar o CSV e exibir a primeira linha em formato de tabela
update_csv_with_new_entities_and_display_table(csv_file_path)

"""# O último passo para compreender como as linguagens computacionais influenciam a hermenêutica historiográfica está direcionado à interpretação in loco da documentação. O código abaixo percorrerá todas as linhas da base de dados e verificará, para o mesmo arquivo, se há mais de um conceito registrado na mesma página. Observe que, para este exercício, que considerou seis arquivos históricos distintos (IHGB, Academia Nacional de História da Argentina, Biblioteca Brasiliana da USP, Biblioteca de Obras Raras da UFRJ, Instituto Ibero-Americano de Berlim e Hemeroteca Digital), foram encontradas mais de 8 mil interseções"""

import pandas as pd
from collections import defaultdict

# Função para comparar conceitos na mesma página do mesmo documento usando arquivo XLSX
def compare_concepts_on_same_page_xlsx(file_path):
    # Carregar o arquivo XLSX em um DataFrame
    df = pd.read_excel(file_path)

    # Criar um dicionário para agrupar os conceitos por 'Nome do documento' e 'Página'
    concepts_per_page = defaultdict(list)

    # Preencher o dicionário com os dados
    for _, row in df.iterrows():
        nome_documento = row['Nome do documento']
        pagina = row['Página']
        conceito = row['Segmento']
        concepts_per_page[(nome_documento, pagina)].append(conceito)

    # Criar uma lista de páginas com mais de um conceito no mesmo documento
    pages_with_multiple_concepts = []
    for (nome_documento, pagina), conceitos in concepts_per_page.items():
        if len(conceitos) > 1:
            pages_with_multiple_concepts.append({
                "Nome do documento": nome_documento,
                "Página": pagina,
                "Conceitos": ', '.join(conceitos)
            })

    # Criar um DataFrame com os resultados
    result_df = pd.DataFrame(pages_with_multiple_concepts)

    # Exibir todos os resultados
    if not result_df.empty:
        print("Páginas com múltiplos conceitos:")
        print(result_df.to_string())  # Exibe todo o DataFrame no console
    else:
        print("Nenhuma página com múltiplos conceitos encontrada.")

# Exemplo de uso - Caminho para o arquivo XLSX
xlsx_file_path = '/content/conceitos v1.2.xlsx'  # Substitua pelo caminho do seu arquivo XLSX

# Executar a função para comparar conceitos na mesma página
compare_concepts_on_same_page_xlsx(xlsx_file_path)

"""## A forma mais indicada de explorar os dados é a própria leitura deles a partir dos endereços gerados pelo rastramento. Mas ainda é possível fazer filtros analíicos, a partir de datas, periódicos, padrões, leituras bibliográficas. Os gráficos abaixos servem como exemplos para explorar os dados. Nota-se a relevância da Revista Americana e o II Congresso Internacinoal de História da América."""

import pandas as pd
import matplotlib.pyplot as plt

# Função para carregar os resultados e criar gráficos
def plot_analysis(file_path):
    # Carregar o arquivo XLSX em um DataFrame
    df = pd.read_excel(file_path)

    # Gráfico 1: Top 10 documentos com mais resultados
    top_docs = df['Nome do documento'].value_counts().head(20)
    plt.figure(figsize=(20, 6))
    top_docs.plot(kind='bar', color='blue')
    plt.title('20 Documentos com mais interseções')
    plt.xlabel('Nome do Documento')
    plt.ylabel('Número de Resultados')
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

    # Exemplo de uso - Caminho para o arquivo XLSX
xlsx_file_path = '/content/conceitos v1.2.xlsx'  # Substitua pelo caminho do seu arquivo XLSX

# Executar a função para criar os gráficos
plot_analysis(xlsx_file_path)

import pandas as pd
import matplotlib.pyplot as plt
from collections import defaultdict

# Função para carregar os resultados e criar gráficos
def plot_intersections_by_year(file_path):
    # Carregar o arquivo XLSX em um DataFrame
    df = pd.read_excel(file_path)

    # Criar um dicionário para agrupar os conceitos por 'Ano', 'Nome do documento' e 'Página'
    intersections_per_year = defaultdict(int)

    # Preencher o dicionário com os dados agrupados por ano e páginas com mais de um conceito
    for _, row in df.iterrows():
        nome_documento = row['Nome do documento']
        pagina = row['Página']
        ano = row['Ano']
        conceito = row['Segmento']
        intersections_per_year[(ano, nome_documento, pagina)] += 1

    # Contar o número de intersecções de conceitos por ano
    intersections_count = defaultdict(int)
    for (ano, nome_documento, pagina), count in intersections_per_year.items():
        if count > 1:  # Considerar apenas páginas com mais de um conceito
            intersections_count[ano] += 1

    # Converter para DataFrame para visualização
    df_intersections = pd.DataFrame(list(intersections_count.items()), columns=['Ano', 'Número de Intersecções'])

    # Ordenar pelos anos com mais intersecções
    df_intersections = df_intersections.sort_values(by='Número de Intersecções', ascending=False)

    # Gráfico: Anos com mais intersecções de conceitos
    plt.figure(figsize=(10, 6))
    plt.bar(df_intersections['Ano'], df_intersections['Número de Intersecções'], color='purple')
    plt.title('Anos com Mais Intersecções de Conceitos')
    plt.xlabel('Ano')
    plt.ylabel('Número de Intersecções de Conceitos')
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

# Exemplo de uso - Caminho para o arquivo XLSX
xlsx_file_path = '/content/conceitos v1.2.xlsx'  # Substitua pelo caminho do seu arquivo XLSX

# Executar a função para criar o gráfico de intersecções por ano
plot_intersections_by_year(xlsx_file_path)

